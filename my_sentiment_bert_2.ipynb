{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aalen\\OneDrive\\Ê°åÈù¢\\deep learning\\.tfvenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "dataset = load_dataset(\"imdb\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25000/25000 [00:04<00:00, 5602.77 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation= True,\n",
    "        padding = \"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function,batched = True)\n",
    "\n",
    "tokenized_dataset.set_format(\"torch\", columns = [\"input_ids\",\"attention_mask\",\"label\"])\n",
    "\n",
    "tokenized_train_data = tokenized_dataset[\"train\"]\n",
    "tokenized_test_data = tokenized_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aalen\\OneDrive\\Ê°åÈù¢\\deep learning\\.tfvenv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\aalen\\AppData\\Local\\Temp\\ipykernel_9072\\2673237739.py:12: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      " 11%|‚ñà         | 500/4689 [03:44<31:39,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3197, 'grad_norm': 11.426033973693848, 'learning_rate': 1.7867349114949884e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 11%|‚ñà         | 500/4689 [07:24<31:39,  2.21it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24071145057678223, 'eval_runtime': 219.8311, 'eval_samples_per_second': 113.724, 'eval_steps_per_second': 7.11, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|‚ñà‚ñà‚ñè       | 1000/4689 [11:12<27:51,  2.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2526, 'grad_norm': 9.642156600952148, 'learning_rate': 1.5734698229899766e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 21%|‚ñà‚ñà‚ñè       | 1000/4689 [14:53<27:51,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20636381208896637, 'eval_runtime': 220.3785, 'eval_samples_per_second': 113.441, 'eval_steps_per_second': 7.092, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|‚ñà‚ñà‚ñà‚ñè      | 1500/4689 [18:39<23:58,  2.22it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2228, 'grad_norm': 13.268056869506836, 'learning_rate': 1.3602047344849649e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 32%|‚ñà‚ñà‚ñà‚ñè      | 1500/4689 [22:20<23:58,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20226521790027618, 'eval_runtime': 220.4291, 'eval_samples_per_second': 113.415, 'eval_steps_per_second': 7.091, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 2000/4689 [26:06<20:16,  2.21it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1651, 'grad_norm': 7.693418025970459, 'learning_rate': 1.1469396459799531e-05, 'epoch': 1.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 2000/4689 [29:45<20:16,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25927668809890747, 'eval_runtime': 219.2337, 'eval_samples_per_second': 114.034, 'eval_steps_per_second': 7.129, 'epoch': 1.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 2500/4689 [33:29<16:15,  2.24it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1466, 'grad_norm': 21.2023983001709, 'learning_rate': 9.336745574749414e-06, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 2500/4689 [37:28<16:15,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2298092544078827, 'eval_runtime': 238.3126, 'eval_samples_per_second': 104.904, 'eval_steps_per_second': 6.559, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 3000/4689 [41:12<12:32,  2.24it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1492, 'grad_norm': 15.564982414245605, 'learning_rate': 7.204094689699297e-06, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 3000/4689 [44:49<12:32,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24223186075687408, 'eval_runtime': 217.485, 'eval_samples_per_second': 114.95, 'eval_steps_per_second': 7.187, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 3500/4689 [48:33<08:49,  2.25it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0995, 'grad_norm': 25.693227767944336, 'learning_rate': 5.07144380464918e-06, 'epoch': 2.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 3500/4689 [53:14<08:49,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2978869080543518, 'eval_runtime': 281.1337, 'eval_samples_per_second': 88.926, 'eval_steps_per_second': 5.56, 'epoch': 2.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 4000/4689 [56:59<05:07,  2.24it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.086, 'grad_norm': 0.04625948891043663, 'learning_rate': 2.9387929195990615e-06, 'epoch': 2.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 4000/4689 [1:00:36<05:07,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28855082392692566, 'eval_runtime': 217.0359, 'eval_samples_per_second': 115.188, 'eval_steps_per_second': 7.202, 'epoch': 2.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 4500/4689 [1:04:20<01:23,  2.25it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0947, 'grad_norm': 0.06217218190431595, 'learning_rate': 8.061420345489445e-07, 'epoch': 2.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 4500/4689 [1:07:57<01:23,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28094425797462463, 'eval_runtime': 217.2379, 'eval_samples_per_second': 115.081, 'eval_steps_per_second': 7.195, 'epoch': 2.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4689/4689 [1:09:24<00:00,  1.13it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 4164.8377, 'train_samples_per_second': 18.008, 'train_steps_per_second': 1.126, 'train_loss': 0.16769915690781034, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4689, training_loss=0.16769915690781034, metrics={'train_runtime': 4164.8377, 'train_samples_per_second': 18.008, 'train_steps_per_second': 1.126, 'total_flos': 9935054899200000.0, 'train_loss': 0.16769915690781034, 'epoch': 3.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./finetuned\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps = 500,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate= 2e-5,\n",
    "    per_device_eval_batch_size=16,\n",
    "    per_device_train_batch_size=16,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset= tokenized_train_data,\n",
    "    eval_dataset= tokenized_test_data,\n",
    "    tokenizer = tokenizer\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input Text 1: The ending of the show was absolutely terrible, it pretty much ruined the entire show. It was great until episode 7, but when they made Sang-woo betray Ali, and the main character ruin his friendship with Il-nam it quickly became one of the most horribly directed episodes I have ever watched. Plus, they purposefully showed us the good side or the soft side of some main characters just to kill them off in the most meaningless and uneventful way possible? And the whole secret detective thing on the side was such bullsh1t. At first, you think, damn, he may be onto something what a great guy, then, in the end, he ends up shaking hands with the bad guys, and now suddenly all his good intentions and aim to expose these VIP's and the Game/Front Man (which he risked his life for) are gone??? If you enjoy characters dying then coming back to life, or the winner of the game continuing to spend his life like a homeless bum loser even after winning 45 billion won, this show is for you. In case you happen to want to save your time and energy for a better show that actually makes sense, then I suggest you don't watch this show. It's definitely NOT worth the hype.\n",
      "Predicted Label: NEGATIVE (Confidence: 1.00)\n",
      "\n",
      "Input Text 2: So I first started watching this awesome season through my brother suggesting it too me! And what a beautiful suggestion it was. My oh my goodness this season excited me and to be brutally honest first episode I didn‚Äôt have much clue to what it was about, I hadn‚Äôt even watched a trailer for it and to be honest I don‚Äôt really like the name of the show Squid Game. But the a old saying in life don‚Äôt judge a book by its cover and I guess that old saying rings true here.\n",
      "Predicted Label: POSITIVE (Confidence: 0.99)\n",
      "\n",
      "Input Text 3: No wonder this series took so long to create and it shows because there's literally not one flaw. It's perfect in every sense and has every major ingredient that makes it one of the best shows of this decade hands down! The acting; absolutely incredible. The leads acting is stunning and beautiful even without him saying anything his facial expressions speaks volumes in VERY dramatic scenes (which scenes aren't?)\n",
      "Predicted Label: POSITIVE (Confidence: 1.00)\n"
     ]
    }
   ],
   "source": [
    "# Test input\n",
    "input_text = [\n",
    "    \"The ending of the show was absolutely terrible, it pretty much ruined the entire show. It was great until episode 7, but when they made Sang-woo betray Ali, and the main character ruin his friendship with Il-nam it quickly became one of the most horribly directed episodes I have ever watched. Plus, they purposefully showed us the good side or the soft side of some main characters just to kill them off in the most meaningless and uneventful way possible? And the whole secret detective thing on the side was such bullsh1t. At first, you think, damn, he may be onto something what a great guy, then, in the end, he ends up shaking hands with the bad guys, and now suddenly all his good intentions and aim to expose these VIP's and the Game/Front Man (which he risked his life for) are gone??? If you enjoy characters dying then coming back to life, or the winner of the game continuing to spend his life like a homeless bum loser even after winning 45 billion won, this show is for you. In case you happen to want to save your time and energy for a better show that actually makes sense, then I suggest you don't watch this show. It's definitely NOT worth the hype.\",\n",
    "    \"So I first started watching this awesome season through my brother suggesting it too me! And what a beautiful suggestion it was. My oh my goodness this season excited me and to be brutally honest first episode I didn‚Äôt have much clue to what it was about, I hadn‚Äôt even watched a trailer for it and to be honest I don‚Äôt really like the name of the show Squid Game. But the a old saying in life don‚Äôt judge a book by its cover and I guess that old saying rings true here.\",\n",
    "    \"No wonder this series took so long to create and it shows because there's literally not one flaw. It's perfect in every sense and has every major ingredient that makes it one of the best shows of this decade hands down! The acting; absolutely incredible. The leads acting is stunning and beautiful even without him saying anything his facial expressions speaks volumes in VERY dramatic scenes (which scenes aren't?)\",\n",
    "]\n",
    "\n",
    "# Tokenize the new data\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "device = model.device\n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "# Pass the tokenized inputs through the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract predictions\n",
    "predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "label_map = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "\n",
    "# Display predictions with confidence scores\n",
    "import torch.nn.functional as F\n",
    "probabilities = F.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "for i, predicted_label in enumerate(predicted_labels):\n",
    "    churn_label = label_map[predicted_label]\n",
    "    confidence = probabilities[i][predicted_label].item()\n",
    "    print(f\"\\nInput Text {i + 1}: {input_text[i]}\")\n",
    "    print(f\"Predicted Label: {churn_label} (Confidence: {confidence:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aalen\\AppData\\Local\\Temp\\ipykernel_9072\\770955244.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = {key: torch.tensor(val).to(model.device) for key, val in batch.items() if key in [\"input_ids\", \"attention_mask\"]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9286\n"
     ]
    }
   ],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(batch):\n",
    "    inputs = {key: torch.tensor(val).to(model.device) for key, val in batch.items() if key in [\"input_ids\", \"attention_mask\"]}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=1)\n",
    "    return predictions.cpu().numpy()\n",
    "\n",
    "tokenized_test_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "for batch in torch.utils.data.DataLoader(tokenized_test_data, batch_size=32):\n",
    "    predictions = compute_metrics(batch)\n",
    "    all_predictions.extend(predictions)\n",
    "    all_labels.extend(batch[\"label\"].numpy())\n",
    "\n",
    "# Compute accuracy\n",
    "results = metric.compute(predictions=all_predictions, references=all_labels)\n",
    "print(f\"Accuracy: {results['accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tfvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
